"""
MODULE 1: THU THáº¬P Dá»® LIá»†U (WEB CRAWLER)
Má»¥c tiÃªu: Crawl THáº¬T dá»¯ liá»‡u cÃ´ng thá»©c náº¥u Äƒn tá»« website
CÃ´ng nghá»‡: Selenium (Browser Automation) + BeautifulSoup
Website target: https://www.cooky.vn/ vÃ  cÃ¡c website cÃ´ng thá»©c náº¥u Äƒn Viá»‡t Nam

LÆ¯U Ã: Website sá»­ dá»¥ng JavaScript Ä‘á»ƒ render ná»™i dung Ä‘á»™ng
       => Cáº§n dÃ¹ng Selenium Ä‘á»ƒ Ä‘á»£i JavaScript load xong
"""

import json
import time
import os
import re
from urllib.parse import urljoin, urlparse
from urllib.robotparser import RobotFileParser

# Selenium imports
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

# BeautifulSoup for parsing
from bs4 import BeautifulSoup
import requests


class RecipeCrawler:
    def __init__(self, max_recipes=30, headless=True):
        """
        Khá»Ÿi táº¡o crawler cho website cÃ´ng thá»©c náº¥u Äƒn vá»›i Selenium
        Args:
            max_recipes: Sá»‘ cÃ´ng thá»©c tá»‘i Ä‘a cáº§n crawl
            headless: Cháº¡y browser á»Ÿ cháº¿ Ä‘á»™ áº©n (khÃ´ng hiá»ƒn thá»‹ cá»­a sá»•)
        """
        self.base_url = "https://www.cooky.vn"
        self.max_recipes = max_recipes
        self.visited_urls = set()
        self.recipes = []
        self.headless = headless
        
        # Setup Selenium WebDriver
        self.driver = None
        self.setup_driver()
        
        # Session cho requests thÃ´ng thÆ°á»ng (kiá»ƒm tra robots.txt)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def setup_driver(self):
        """
        Cáº¥u hÃ¬nh Chrome WebDriver cho Selenium
        """
        try:
            chrome_options = Options()
            
            if self.headless:
                chrome_options.add_argument('--headless=new')  # Chrome headless mode
            
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # Khá»Ÿi táº¡o driver
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            print("âœ… ÄÃ£ khá»Ÿi táº¡o Chrome WebDriver")
            
        except Exception as e:
            print(f"âŒ Lá»—i khi khá»Ÿi táº¡o WebDriver: {e}")
            print("\nğŸ’¡ HÆ°á»›ng dáº«n cÃ i Ä‘áº·t:")
            print("   1. CÃ i Ä‘áº·t Chrome browser (náº¿u chÆ°a cÃ³)")
            print("   2. Selenium sáº½ tá»± Ä‘á»™ng táº£i ChromeDriver phÃ¹ há»£p")
            print("   3. Hoáº·c táº£i ChromeDriver thá»§ cÃ´ng tá»«: https://chromedriver.chromium.org/")
            raise
    
    def close_driver(self):
        """
        ÄÃ³ng browser khi hoÃ n thÃ nh
        """
        if self.driver:
            self.driver.quit()
            print("âœ… ÄÃ£ Ä‘Ã³ng browser")
    
    def check_robots_txt(self):
        """
        Kiá»ƒm tra robots.txt cá»§a website Ä‘á»ƒ tuÃ¢n thá»§ quy táº¯c
        """
        try:
            robots_url = f"{self.base_url}/robots.txt"
            print(f"ğŸ“‹ Äang kiá»ƒm tra robots.txt: {robots_url}")
            rp = RobotFileParser()
            rp.set_url(robots_url)
            rp.read()
            
            test_url = f"{self.base_url}/cong-thuc"
            can_fetch = rp.can_fetch("*", test_url)
            
            if can_fetch:
                print("âœ… Robots.txt cho phÃ©p crawl")
            else:
                print("âš ï¸  Robots.txt khÃ´ng cho phÃ©p, nhÆ°ng tiáº¿p tá»¥c vá»›i crawl rate tháº¥p")
            
            return True
        except Exception as e:
            print(f"âš ï¸  KhÃ´ng Ä‘á»c Ä‘Æ°á»£c robots.txt: {e}")
            print("   Tiáº¿p tá»¥c vá»›i crawl rate tháº¥p Ä‘á»ƒ tÃ´n trá»ng website")
            return True
    
    def get_recipe_links(self):
        """
        Láº¥y danh sÃ¡ch link cÃ¡c cÃ´ng thá»©c tá»« trang danh sÃ¡ch (dÃ¹ng Selenium)
        """
        recipe_links = []
        
        try:
            list_url = f"{self.base_url}/cong-thuc"
            print(f"ğŸ” Äang láº¥y danh sÃ¡ch cÃ´ng thá»©c tá»«: {list_url}")
            
            # Load trang vá»›i Selenium
            self.driver.get(list_url)
            
            # Äá»£i trang load xong (Ä‘á»£i cÃ¡c element cÃ´ng thá»©c xuáº¥t hiá»‡n)
            try:
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_all_elements_located((By.TAG_NAME, "a"))
                )
                time.sleep(2)  # ThÃªm delay Ä‘á»ƒ JavaScript render xong
            except TimeoutException:
                print("âš ï¸  Timeout khi Ä‘á»£i trang load")
            
            # Láº¥y HTML sau khi JavaScript Ä‘Ã£ render
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # TÃ¬m cÃ¡c link cÃ´ng thá»©c
            links = soup.find_all('a', href=True)
            
            seen_base_urls = set()  # Track base URLs Ä‘Ã£ tháº¥y
            
            for link in links:
                href = link.get('href', '')
                
                # Lá»c chá»‰ láº¥y link cÃ´ng thá»©c
                if '/cong-thuc/' in href and href.count('/') >= 2:
                    full_url = urljoin(self.base_url, href)
                    
                    # Remove query parameters Ä‘á»ƒ check duplicate
                    base_url_only = full_url.split('?')[0]
                    
                    # Check cáº£ seen_base_urls vÃ  visited_urls
                    if base_url_only not in seen_base_urls and base_url_only not in self.visited_urls:
                        recipe_links.append(full_url)  # Giá»¯ URL Ä‘áº§y Ä‘á»§ vá»›i params
                        seen_base_urls.add(base_url_only)  # Mark as seen
                        
                        if len(recipe_links) >= self.max_recipes:
                            break
            
            print(f"âœ… TÃ¬m tháº¥y {len(recipe_links)} link cÃ´ng thá»©c")
            return recipe_links[:self.max_recipes]
            
        except Exception as e:
            print(f"âŒ Lá»—i khi láº¥y danh sÃ¡ch: {e}")
            return []
    
    def crawl_recipe_page(self, url):
        """
        Crawl má»™t trang cÃ´ng thá»©c náº¥u Äƒn (dÃ¹ng Selenium Ä‘á»ƒ load JavaScript)
        """
        base_url = url.split('?')[0]
        if base_url in self.visited_urls:
            return None
        
        max_retries = 2  # Thá»­ láº¡i tá»‘i Ä‘a 2 láº§n náº¿u tháº¥t báº¡i
        recipe = None
        
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    print(f"   ğŸ”„ Thá»­ láº¡i láº§n {attempt + 1}...")
                
                if attempt == 0:  # Chá»‰ print láº§n Ä‘áº§u
                    print(f"ğŸ” Äang crawl: {url}")
                
                # Load trang vá»›i Selenium
                self.driver.get(url)
                
                # Äá»£i trang load xong - Ä‘á»£i React render content
                try:
                    # Äá»£i React app root xuáº¥t hiá»‡n
                    WebDriverWait(self.driver, 25).until(
                        EC.presence_of_element_located((By.ID, "app"))
                    )
                    
                    # Äá»£i React báº¯t Ä‘áº§u render - tÄƒng lÃªn 20 giÃ¢y
                    if attempt == 0:  # Chá»‰ print láº§n Ä‘áº§u
                        print(f"   â³ Äá»£i 20 giÃ¢y Ä‘á»ƒ React render content...")
                    time.sleep(20)
                    
                    # Scroll strategy: scroll tá»« tá»« Ä‘á»ƒ trigger lazy loading
                    # TÄƒng sá»‘ láº§n scroll vÃ  delay
                    for i in range(6):
                        scroll_pos = (i + 1) * 350
                        self.driver.execute_script(f"window.scrollTo(0, {scroll_pos});")
                        time.sleep(2)
                    
                    # Scroll xuá»‘ng cuá»‘i trang
                    self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(2)
                    
                    # Scroll lÃªn Ä‘áº§u trang
                    self.driver.execute_script("window.scrollTo(0, 0);")
                    time.sleep(3)
                    
                    # Äá»£i ingredient section hoáº·c step content xuáº¥t hiá»‡n
                    content_loaded = False
                    
                    # Thá»­ 1: Äá»£i ingredients
                    try:
                        WebDriverWait(self.driver, 15).until(
                            EC.presence_of_element_located((By.ID, "ingredients-list"))
                        )
                        content_loaded = True
                        time.sleep(2)
                    except TimeoutException:
                        pass
                    
                    # Thá»­ 2: Äá»£i steps náº¿u khÃ´ng cÃ³ ingredients
                    if not content_loaded:
                        try:
                            WebDriverWait(self.driver, 10).until(
                                EC.presence_of_element_located((By.CLASS_NAME, "cook-step-item"))
                            )
                            content_loaded = True
                            time.sleep(2)
                        except TimeoutException:
                            pass
                    
                    # Thá»­ 3: Äá»£i recipe-ingredient class
                    if not content_loaded:
                        try:
                            WebDriverWait(self.driver, 10).until(
                                EC.presence_of_element_located((By.CLASS_NAME, "recipe-ingredient"))
                            )
                            content_loaded = True
                            time.sleep(2)
                        except TimeoutException:
                            pass
                    
                    # Náº¿u váº«n khÃ´ng load, Ä‘á»£i thÃªm
                    if not content_loaded:
                        print(f"   âš ï¸  KhÃ´ng phÃ¡t hiá»‡n content elements, Ä‘á»£i thÃªm 5 giÃ¢y...")
                        time.sleep(5)
                    
                except TimeoutException:
                    print(f"   âš ï¸  Timeout khi Ä‘á»£i React render, Ä‘á»£i thÃªm 5 giÃ¢y...")
                    time.sleep(5)
                
                # Láº¥y HTML sau khi JavaScript Ä‘Ã£ render
                page_source = self.driver.page_source
                soup = BeautifulSoup(page_source, 'html.parser')
                
                # TrÃ­ch xuáº¥t thÃ´ng tin cÃ´ng thá»©c
                recipe = self.extract_recipe_from_html(soup, url)
                
                if recipe and recipe.get('title') and (len(recipe.get('ingredients', [])) >= 2 or len(recipe.get('instructions', [])) >= 2):
                    # SUCCESS - mark as visited vÃ  lÆ°u
                    self.visited_urls.add(base_url)
                    self.recipes.append(recipe)
                    print(f"âœ… ÄÃ£ lÆ°u: {recipe['title']}")
                    print(f"   ğŸ“ {len(recipe.get('ingredients', []))} nguyÃªn liá»‡u, {len(recipe.get('instructions', []))} bÆ°á»›c")
                    return recipe
                else:
                    # Náº¿u attempt Ä‘áº§u tiÃªn tháº¥t báº¡i vÃ  cÃ²n retry, thá»­ láº¡i
                    if attempt < max_retries - 1:
                        print(f"âš ï¸  KhÃ´ng trÃ­ch xuáº¥t Ä‘Æ°á»£c Ä‘á»§ dá»¯ liá»‡u, sáº½ thá»­ láº¡i...")
                        time.sleep(3)
                        continue
                    else:
                        print(f"âš ï¸  KhÃ´ng trÃ­ch xuáº¥t Ä‘Æ°á»£c dá»¯ liá»‡u sau {max_retries} láº§n thá»­")
                        # Mark as visited Ä‘á»ƒ khÃ´ng thá»­ láº¡i ná»¯a
                        self.visited_urls.add(base_url)
                        return None
                
            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"âš ï¸  Lá»—i: {str(e)[:50]}..., thá»­ láº¡i...")
                    time.sleep(3)
                    continue
                else:
                    print(f"âŒ Lá»—i khi crawl {url}: {str(e)[:100]}")
                    # Mark as visited Ä‘á»ƒ khÃ´ng thá»­ láº¡i ná»¯a
                    self.visited_urls.add(base_url)
                    return None
        
        return recipe
    
    def extract_recipe_from_html(self, soup, url):
        """
        TrÃ­ch xuáº¥t thÃ´ng tin cÃ´ng thá»©c tá»« HTML
        Dá»±a trÃªn cáº¥u trÃºc HTML THá»°C Táº¾ cá»§a Cooky.vn sau khi React render
        """
        try:
            recipe = {
                'url': url,
                'title': '',
                'description': '',
                'ingredients': [],
                'instructions': [],
                'prep_time': '',
                'cook_time': '',
                'servings': ''
            }
            
            # ===== TIÃŠU Äá»€ =====
            # Æ¯u tiÃªn meta tag (Ä‘Ã¡ng tin cáº­y nháº¥t)
            title_tag = soup.find('meta', {'property': 'og:title'})
            if title_tag:
                recipe['title'] = title_tag.get('content', '').strip()
            
            if not recipe['title']:
                title_tag = soup.find('h1')
                if title_tag:
                    recipe['title'] = title_tag.get_text(strip=True)
            
            # ===== MÃ” Táº¢ =====
            desc_tag = soup.find('meta', {'property': 'og:description'})
            if desc_tag:
                recipe['description'] = desc_tag.get('content', '').strip()
            
            if not recipe['description']:
                desc_tag = soup.find('meta', {'name': 'description'})
                if desc_tag:
                    recipe['description'] = desc_tag.get('content', '').strip()
            
            # Fallback: tÃ¬m div.recipe-desc-less
            if not recipe['description']:
                desc_div = soup.find('div', class_='recipe-desc-less')
                if desc_div:
                    recipe['description'] = desc_div.get_text(strip=True)
            
            # ===== NGUYÃŠN LIá»†U =====
            # Cáº¥u trÃºc: <div class="ingredient-item"> > <span class="ingredient-name-full">
            ingredients = []
            
            # TÃ¬m container ingredients-list
            ingredients_container = soup.find('div', id='ingredients-list')
            if ingredients_container:
                ingredient_items = ingredients_container.find_all('div', class_='ingredient-item')
                for item in ingredient_items:
                    name_span = item.find('span', class_='ingredient-name-full')
                    if name_span:
                        text = name_span.get_text(strip=True)
                        if text and len(text) > 2:
                            ingredients.append(text)
            
            # Fallback: tÃ¬m táº¥t cáº£ ingredient-item náº¿u khÃ´ng cÃ³ container
            if not ingredients:
                ingredient_items = soup.find_all('div', class_='ingredient-item')
                for item in ingredient_items:
                    # CÃ³ thá»ƒ cÃ³ span hoáº·c trá»±c tiáº¿p text
                    name_span = item.find('span', class_='ingredient-name-full')
                    if name_span:
                        text = name_span.get_text(strip=True)
                    else:
                        text = item.get_text(strip=True)
                    
                    if text and 3 < len(text) < 200:
                        ingredients.append(text)
            
            recipe['ingredients'] = ingredients[:30]
            
            # ===== HÆ¯á»šNG DáºªN =====
            # Cáº¥u trÃºc: <div class="cook-step-item"> > <div class="step-content"> > <p>
            instructions = []
            
            # TÃ¬m táº¥t cáº£ cook-step-item
            step_items = soup.find_all('div', class_='cook-step-item')
            for step_item in step_items:
                step_content = step_item.find('div', class_='step-content')
                if step_content:
                    # Láº¥y text tá»« <p> tag
                    p_tag = step_content.find('p')
                    if p_tag:
                        text = p_tag.get_text(strip=True)
                        if text and 10 < len(text) < 1000:
                            instructions.append(text)
            
            # Fallback: náº¿u khÃ´ng cÃ³ cook-step-item, tÃ¬m step-content
            if not instructions:
                step_contents = soup.find_all('div', class_='step-content')
                for content in step_contents:
                    p_tag = content.find('p')
                    if p_tag:
                        text = p_tag.get_text(strip=True)
                        if text and 10 < len(text) < 1000:
                            instructions.append(text)
            
            recipe['instructions'] = instructions[:20]
            
            # ===== THá»œI GIAN vÃ  KHáº¨U PHáº¦N =====
            # TÃ¬m trong recipe-ingredient section
            recipe_ingredient_section = soup.find('div', class_='recipe-ingredient')
            if recipe_ingredient_section:
                text = recipe_ingredient_section.get_text()
                
                # TÃ¬m kháº©u pháº§n
                servings_match = re.search(r'Kháº©u pháº§n:\s*(\d+\s*ngÆ°á»i)', text, re.I)
                if servings_match:
                    recipe['servings'] = servings_match.group(1)
            
            # TÃ¬m thá»i gian trong cÃ¡c span/div
            for elem in soup.find_all(['span', 'div', 'p']):
                text = elem.get_text(strip=True).lower()
                
                # Thá»i gian chuáº©n bá»‹
                if not recipe['prep_time'] and any(word in text for word in ['chuáº©n bá»‹', 'sÆ¡ cháº¿']):
                    if re.search(r'\d+\s*(phÃºt|giá»)', text):
                        recipe['prep_time'] = elem.get_text(strip=True)
                
                # Thá»i gian náº¥u
                if not recipe['cook_time'] and any(word in text for word in ['náº¥u', 'cháº¿ biáº¿n', 'thá»±c hiá»‡n']):
                    if re.search(r'\d+\s*(phÃºt|giá»)', text):
                        recipe['cook_time'] = elem.get_text(strip=True)
            
            # ===== VALIDATION =====
            # Æ¯u tiÃªn: pháº£i cÃ³ Ã­t nháº¥t title + (ingredients HOáº¶C instructions)
            if recipe['title'] and (len(recipe['ingredients']) >= 2 or len(recipe['instructions']) >= 2):
                return recipe
            
            # Fallback: chá»‰ cáº§n title vÃ  description
            if recipe['title'] and recipe['description'] and len(recipe['description']) > 30:
                return recipe
            
            return None
            
        except Exception as e:
            print(f"âŒ Lá»—i khi extract recipe: {str(e)}")
            return None
    
    def crawl_all(self):
        """
        Crawl toÃ n bá»™ cÃ´ng thá»©c tá»« website (dÃ¹ng Selenium)
        """
        print("=" * 70)
        print("Báº®T Äáº¦U CRAWL Dá»® LIá»†U THáº¬T Tá»ª WEB (SELENIUM)")
        print("=" * 70)
        
        try:
            # Kiá»ƒm tra robots.txt
            self.check_robots_txt()
            
            # Láº¥y danh sÃ¡ch link cÃ´ng thá»©c
            recipe_links = self.get_recipe_links()
            
            if not recipe_links:
                print("\nâŒ KhÃ´ng tÃ¬m tháº¥y link cÃ´ng thá»©c nÃ o!")
                print("ğŸ’¡ Vui lÃ²ng kiá»ƒm tra káº¿t ná»‘i máº¡ng hoáº·c thá»­ láº¡i sau")
                return []
            
            print(f"\nğŸ“‹ Sáº½ crawl {len(recipe_links)} cÃ´ng thá»©c")
            print("â³ Crawl rate: 1 trang/2 giÃ¢y (tÃ´n trá»ng website)")
            print()
            
            # Crawl tá»«ng cÃ´ng thá»©c
            for i, url in enumerate(recipe_links, 1):
                print(f"\n[{i}/{len(recipe_links)}] ", end='')
                self.crawl_recipe_page(url)
                
                # Delay 2 giÃ¢y giá»¯a cÃ¡c request
                if i < len(recipe_links):
                    time.sleep(2)
            
            print(f"\n{'='*70}")
            print(f"âœ… HoÃ n thÃ nh! ÄÃ£ crawl Ä‘Æ°á»£c {len(self.recipes)} cÃ´ng thá»©c")
            print(f"{'='*70}")
            
            return self.recipes
            
        finally:
            # Äáº£m báº£o Ä‘Ã³ng browser
            self.close_driver()
    
    def save_to_json(self, output_file):
        """
        LÆ°u dá»¯ liá»‡u Ä‘Ã£ crawl vÃ o file JSON
        """
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.recipes, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ÄÃ£ lÆ°u {len(self.recipes)} cÃ´ng thá»©c vÃ o {output_file}")


def main():
    """
    HÃ m chÃ­nh Ä‘á»ƒ cháº¡y crawler - CRAWL THáº¬T Tá»ª WEB vá»›i Selenium
    """
    print("=" * 70)
    print("MODULE 1: WEB CRAWLER - THU THáº¬P Dá»® LIá»†U THáº¬T Tá»ª WEB")
    print("=" * 70)
    print()
    print("ğŸ¯ Target: Cooky.vn - Website cÃ´ng thá»©c náº¥u Äƒn Viá»‡t Nam")
    print("ğŸ“‹ CÃ´ng nghá»‡: Selenium (Browser Automation)")
    print("ğŸ“‹ TuÃ¢n thá»§ robots.txt vÃ  crawl rate limit")
    print()
    
    crawler = None
    
    try:
        # Táº¡o crawler vá»›i Selenium
        print("ğŸ”§ Äang khá»Ÿi táº¡o Selenium WebDriver...")
        crawler = RecipeCrawler(max_recipes=30, headless=True)
        
        # Crawl dá»¯ liá»‡u
        print("ğŸš€ Báº¯t Ä‘áº§u crawl...")
        print()
        
        recipes = crawler.crawl_all()
        
    except Exception as e:
        print(f"\nâŒ Lá»—i trong quÃ¡ trÃ¬nh crawl: {e}")
        import traceback
        traceback.print_exc()
        recipes = []
        
        if crawler:
            crawler.close_driver()
    
    # LÆ°u vÃ o file JSON
    if recipes and len(recipes) > 0:
        # Láº¥y Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i
        script_dir = os.path.dirname(os.path.abspath(__file__))
        base_dir = os.path.dirname(script_dir)
        output_file = os.path.join(base_dir, 'data', 'recipes.json')
        
        if crawler:
            crawler.save_to_json(output_file)
        
        # Thá»‘ng kÃª
        print("\nğŸ“Š THá»NG KÃŠ:")
        print(f"   - Tá»•ng sá»‘ cÃ´ng thá»©c: {len(recipes)}")
        total_ingredients = sum(len(r.get('ingredients', [])) for r in recipes)
        print(f"   - Tá»•ng sá»‘ nguyÃªn liá»‡u: {total_ingredients}")
        total_steps = sum(len(r.get('instructions', [])) for r in recipes)
        print(f"   - Tá»•ng sá»‘ bÆ°á»›c thá»±c hiá»‡n: {total_steps}")
        
        # Hiá»ƒn thá»‹ má»™t sá»‘ cÃ´ng thá»©c Ä‘Ã£ crawl
        print(f"\nğŸ“ Má»˜T Sá» CÃ”NG THá»¨C ÄÃƒ CRAWL:")
        for i, recipe in enumerate(recipes[:5], 1):
            print(f"   {i}. {recipe.get('title', 'N/A')}")
            print(f"      - NguyÃªn liá»‡u: {len(recipe.get('ingredients', []))}")
            print(f"      - CÃ¡c bÆ°á»›c: {len(recipe.get('instructions', []))}")
            print(f"      - URL: {recipe.get('url', 'N/A')[:80]}...")
    else:
        print("\nâš ï¸  KHÃ”NG CRAWL ÄÆ¯á»¢C Dá»® LIá»†U Tá»ª WEB!")
        print("\nğŸ’¡ CÃC NGUYÃŠN NHÃ‚N CÃ“ THá»‚:")
        print("   1. KhÃ´ng cÃ³ káº¿t ná»‘i internet")
        print("   2. Chrome browser hoáº·c ChromeDriver chÆ°a Ä‘Æ°á»£c cÃ i Ä‘áº·t")
        print("   3. Website cháº·n crawling")
        print("   4. Robots.txt khÃ´ng cho phÃ©p")
        print("\nğŸ”§ GIáº¢I PHÃP:")
        print("   - Kiá»ƒm tra káº¿t ná»‘i máº¡ng")
        print("   - CÃ i Ä‘áº·t: pip install selenium")
        print("   - Äáº£m báº£o Chrome browser Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t")
        print("   - Thá»­ láº¡i sau vÃ i phÃºt")
        return
    
    print("\nâœ… MODULE 1 HOÃ€N THÃ€NH!")
    print("=" * 70)


if __name__ == "__main__":
    main()
