"""
MODULE 2: Xá»¬ LÃ VÄ‚N Báº¢N & XÃ‚Y Dá»°NG CHá»ˆ Má»¤C
Má»¥c tiÃªu: LÃ m sáº¡ch vÄƒn báº£n vÃ  xÃ¢y dá»±ng Inverted Index
"""

import re
import json
from collections import defaultdict
import math
from underthesea import word_tokenize


class TextProcessor:
    """
    Class xá»­ lÃ½ vÄƒn báº£n tiáº¿ng Viá»‡t
    """
    def __init__(self):
        # Danh sÃ¡ch tá»« dá»«ng tiáº¿ng Viá»‡t
        self.stop_words = set([
            'vÃ ', 'cá»§a', 'lÃ ', 'cÃ³', 'Ä‘Æ°á»£c', 'trong', 'cho', 'vá»›i', 'tá»«', 'má»™t',
            'cÃ¡c', 'nÃ y', 'Ä‘Ã³', 'Ä‘á»ƒ', 'nhá»¯ng', 'bá»Ÿi', 'nhÆ°', 'khi', 'Ä‘Ã£', 'táº¡i',
            'vá»', 'vÃ o', 'ra', 'Ä‘áº¿n', 'lÃªn', 'theo', 'nÃªn', 'nhÆ°ng', 'hoáº·c',
            'thÃ¬', 'sáº½', 'ráº¥t', 'cÅ©ng', 'Ä‘ang', 'bá»‹', 'lÃ m', 'nÃ o', 'ai', 'gÃ¬'
        ])
    
    def tokenize(self, text):
        """
        TÃ¡ch tá»« tiáº¿ng Viá»‡t
        Args:
            text: chuá»—i vÄƒn báº£n cáº§n tÃ¡ch
        Returns:
            list: danh sÃ¡ch cÃ¡c tá»«
        """
        try:
            # Sá»­ dá»¥ng underthesea Ä‘á»ƒ tÃ¡ch tá»« tiáº¿ng Viá»‡t
            tokens = word_tokenize(text, format="text").split()
            return tokens
        except:
            # Fallback: tÃ¡ch Ä‘Æ¡n giáº£n náº¿u underthesea lá»—i
            return text.split()
    
    def normalize(self, text):
        """
        Chuáº©n hÃ³a vÄƒn báº£n: chuyá»ƒn vá» chá»¯ thÆ°á»ng, loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t
        """
        # Chuyá»ƒn vá» chá»¯ thÆ°á»ng
        text = text.lower()
        
        # Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t, chá»‰ giá»¯ chá»¯ cÃ¡i, sá»‘ vÃ  khoáº£ng tráº¯ng
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def remove_stopwords(self, tokens):
        """
        Loáº¡i bá» tá»« dá»«ng
        """
        return [token for token in tokens if token not in self.stop_words]
    
    def process(self, text):
        """
        Xá»­ lÃ½ vÄƒn báº£n hoÃ n chá»‰nh: normalize -> tokenize -> remove stopwords
        """
        # Chuáº©n hÃ³a
        normalized_text = self.normalize(text)
        
        # TÃ¡ch tá»«
        tokens = self.tokenize(normalized_text)
        
        # Loáº¡i bá» tá»« dá»«ng
        filtered_tokens = self.remove_stopwords(tokens)
        
        return filtered_tokens


class InvertedIndex:
    """
    Class xÃ¢y dá»±ng vÃ  quáº£n lÃ½ Inverted Index
    """
    def __init__(self):
        self.index = defaultdict(list)  # {term: [(doc_id, frequency, positions), ...]}
        self.doc_lengths = {}  # {doc_id: length}
        self.doc_count = 0
        self.avg_doc_length = 0
        self.text_processor = TextProcessor()
    
    def add_document(self, doc_id, text, field_weight=1.0):
        """
        ThÃªm tÃ i liá»‡u vÃ o index
        Args:
            doc_id: ID cá»§a tÃ i liá»‡u
            text: ná»™i dung vÄƒn báº£n
            field_weight: trá»ng sá»‘ cá»§a trÆ°á»ng (vÃ­ dá»¥: title cÃ³ trá»ng sá»‘ cao hÆ¡n)
        """
        # Xá»­ lÃ½ vÄƒn báº£n
        tokens = self.text_processor.process(text)
        
        # Äáº¿m táº§n suáº¥t vÃ  vá»‹ trÃ­ cá»§a má»—i tá»«
        term_freq = defaultdict(int)
        term_positions = defaultdict(list)
        
        for position, token in enumerate(tokens):
            term_freq[token] += 1
            term_positions[token].append(position)
        
        # ThÃªm vÃ o inverted index
        for term, freq in term_freq.items():
            weighted_freq = freq * field_weight
            self.index[term].append({
                'doc_id': doc_id,
                'frequency': weighted_freq,
                'positions': term_positions[term]
            })
        
        # LÆ°u Ä‘á»™ dÃ i tÃ i liá»‡u
        self.doc_lengths[doc_id] = len(tokens)
    
    def build_from_documents(self, documents):
        """
        XÃ¢y dá»±ng index tá»« danh sÃ¡ch tÃ i liá»‡u
        Args:
            documents: list cÃ¡c dict chá»©a thÃ´ng tin tÃ i liá»‡u
        """
        print("ğŸ”¨ Äang xÃ¢y dá»±ng Inverted Index...")
        
        for doc in documents:
            doc_id = doc['url']  # Sá»­ dá»¥ng URL lÃ m doc_id
            
            # Index cÃ¡c trÆ°á»ng vá»›i trá»ng sá»‘ khÃ¡c nhau
            # Title cÃ³ trá»ng sá»‘ cao nháº¥t
            self.add_document(doc_id, doc.get('title', ''), field_weight=3.0)
            
            # Description cÃ³ trá»ng sá»‘ trung bÃ¬nh
            self.add_document(doc_id, doc.get('description', ''), field_weight=2.0)
            
            # Ingredients
            ingredients_text = ' '.join(doc.get('ingredients', []))
            self.add_document(doc_id, ingredients_text, field_weight=1.5)
            
            # Instructions
            instructions_text = ' '.join(doc.get('instructions', []))
            self.add_document(doc_id, instructions_text, field_weight=1.0)
        
        self.doc_count = len(documents)
        self.avg_doc_length = sum(self.doc_lengths.values()) / self.doc_count if self.doc_count > 0 else 0
        
        print(f"âœ… ÄÃ£ xÃ¢y dá»±ng index cho {self.doc_count} tÃ i liá»‡u")
        print(f"   - Tá»•ng sá»‘ terms: {len(self.index)}")
        print(f"   - Äá»™ dÃ i tÃ i liá»‡u trung bÃ¬nh: {self.avg_doc_length:.2f} tá»«")
    
    def get_posting_list(self, term):
        """
        Láº¥y posting list cá»§a má»™t term
        """
        processed_term = self.text_processor.process(term)
        if processed_term:
            return self.index.get(processed_term[0], [])
        return []
    
    def get_document_frequency(self, term):
        """
        Láº¥y sá»‘ tÃ i liá»‡u chá»©a term
        """
        return len(self.get_posting_list(term))
    
    def get_idf(self, term):
        """
        TÃ­nh IDF (Inverse Document Frequency)
        IDF = log(N / df)
        """
        df = self.get_document_frequency(term)
        if df == 0:
            return 0
        return math.log(self.doc_count / df)
    
    def save(self, filepath):
        """
        LÆ°u index vÃ o file
        """
        data = {
            'index': dict(self.index),
            'doc_lengths': self.doc_lengths,
            'doc_count': self.doc_count,
            'avg_doc_length': self.avg_doc_length
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ÄÃ£ lÆ°u index vÃ o: {filepath}")
    
    def load(self, filepath):
        """
        Táº£i index tá»« file
        """
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.index = defaultdict(list, data['index'])
        self.doc_lengths = data['doc_lengths']
        self.doc_count = data['doc_count']
        self.avg_doc_length = data['avg_doc_length']
        
        print(f"ğŸ“‚ ÄÃ£ táº£i index tá»«: {filepath}")
        print(f"   - Sá»‘ tÃ i liá»‡u: {self.doc_count}")
        print(f"   - Sá»‘ terms: {len(self.index)}")


def main():
    """
    HÃ m chÃ­nh Ä‘á»ƒ xÃ¢y dá»±ng index
    """
    import os
    
    print("=" * 60)
    print("MODULE 2: Xá»¬ LÃ VÄ‚N Báº¢N & XÃ‚Y Dá»°NG CHá»ˆ Má»¤C")
    print("=" * 60)
    
    # Láº¥y Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i tá»« script location
    script_dir = os.path.dirname(os.path.abspath(__file__))
    base_dir = os.path.dirname(script_dir)
    
    # Äá»c dá»¯ liá»‡u tá»« Module 1
    data_file = os.path.join(base_dir, 'data', 'recipes.json')
    print(f"\nğŸ“‚ Äang Ä‘á»c dá»¯ liá»‡u tá»«: {data_file}")
    
    with open(data_file, 'r', encoding='utf-8') as f:
        documents = json.load(f)
    
    print(f"âœ… ÄÃ£ Ä‘á»c {len(documents)} tÃ i liá»‡u")
    
    # XÃ¢y dá»±ng Inverted Index
    inverted_index = InvertedIndex()
    inverted_index.build_from_documents(documents)
    
    # LÆ°u index
    index_file = os.path.join(base_dir, 'index', 'inverted_index.json')
    os.makedirs(os.path.dirname(index_file), exist_ok=True)
    inverted_index.save(index_file)
    
    # Demo: hiá»ƒn thá»‹ má»™t sá»‘ term
    print("\nğŸ“Š MáºªU INDEX (má»™t sá»‘ term):")
    sample_terms = list(inverted_index.index.keys())[:5]
    for term in sample_terms:
        posting_list = inverted_index.index[term]
        print(f"\n   Term: '{term}'")
        print(f"   - Document Frequency: {len(posting_list)}")
        print(f"   - IDF: {inverted_index.get_idf(term):.4f}")
        if posting_list:
            first_posting = posting_list[0]
            print(f"   - VÃ­ dá»¥: doc_id={first_posting['doc_id'][:50]}..., freq={first_posting['frequency']}")
    
    print("\nâœ… MODULE 2 HOÃ€N THÃ€NH!")
    print("=" * 60)


if __name__ == "__main__":
    main()
